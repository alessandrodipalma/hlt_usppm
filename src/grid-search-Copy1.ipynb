{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2239c1-e1b6-46be-8734-f1a3298fb062",
   "metadata": {},
   "source": [
    "# Pip Wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddb3b1f-8cf9-4ead-895f-9a5768440183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install pytorch_lightning\\n!pip install torchmetrics\\n!pip install tokenizers\\n!pip install transformers\\n!pip install ray[tune]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install pytorch_lightning\n",
    "!pip install torchmetrics\n",
    "!pip install tokenizers\n",
    "!pip install transformers\n",
    "!pip install ray[tune]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc890c3d-d9af-45d0-a1df-36462ffdf4b7",
   "metadata": {
    "id": "ICgrtlaznr7F"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba724d89-aea8-4bf1-bd16-1380bcdce9a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUiP-rVLnr7H",
    "outputId": "79dc16d8-54ad-42df-a406-ef7564341a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# General Libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Type\n",
    "from copy import deepcopy\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule, seed_everything, Trainer, LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.functional import f1_score, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loops.loop import Loop\n",
    "from pytorch_lightning.loops.fit_loop import FitLoop\n",
    "from pytorch_lightning.trainer.states import TrainerFn\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# Ray[Tune]\n",
    "import ray\n",
    "from ray import air\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "\n",
    "# HuggingFace Libraries\n",
    "import tokenizers\n",
    "import transformers \n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990e45de-3f45-42f3-b6d9-37e5bfad651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:03:43,285\tINFO worker.py:1518 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.1</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.0.1', ray_commit='03b6bc7b5a305877501110ec04710a9c57011479', address_info={'node_ip_address': '131.114.50.210', 'raylet_ip_address': '131.114.50.210', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-11-06_20-03-40_682162_359967/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-11-06_20-03-40_682162_359967/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-11-06_20-03-40_682162_359967', 'metrics_export_port': 49905, 'gcs_address': '131.114.50.210:48783', 'address': '131.114.50.210:48783', 'dashboard_agent_listen_port': 52365, 'node_id': '9e2c69859f47fecef5f959d917734d065359c4de00a644218e0a34ac'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_gpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afdc09-2e7d-412d-9cbf-39e104819dad",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5fc28-1a0b-4c86-9da6-e156d4e88d13",
   "metadata": {
    "id": "E6Qw8gpgnr7N"
   },
   "source": [
    "## Configuration Class: notebook-specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca2dbb5-92b6-4d04-b479-223aaeb390d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # General\n",
    "    seed = 42\n",
    "    \n",
    "    # Debug \n",
    "    debug = True\n",
    "    debug_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ccc5e-ced9-4c6b-ba6c-29e7ed9ffbac",
   "metadata": {
    "id": "E6Qw8gpgnr7N"
   },
   "source": [
    "## Configuration Dictionary: trial-specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9985db04-ae1c-492c-9cca-eeac1b209381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a search space!\n",
    "config_dict = {\n",
    "    \"target_size\" : 1,\n",
    "    \"num_workers\" : 8,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 2,\n",
    "    \"n_fold\" : 2,\n",
    "    \"warmup_steps\" : 0,\n",
    "    \"min_lr\" : 1e-6,\n",
    "    \"encoder_lr\" : 2e-5,\n",
    "    \"decoder_lr\" : 2e-5,\n",
    "    \"eps\" : 1e-6,\n",
    "    \"betas\" : (0.9, 0.999),\n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"fc_dropout\" : 0.2,\n",
    "\n",
    "    # Transformers\n",
    "    # \"model\" : tune.grid_search([\"distilbert-base-uncased\", \"microsoft/deberta-v3-large\"]),\n",
    "    # \"model\" : tune.choice([\"microsoft/deberta-v3-large\"]),\n",
    "    # \"model\" : tune.choice([\"distilbert-base-uncased\"]),\n",
    "    \"model\" : \"distilbert-base-uncased\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd0738-9bbe-4d25-8510-aaf5a7110750",
   "metadata": {
    "id": "iIVnbwmdnr7K"
   },
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5da60d-3f9b-4ea6-9254-dc40d402afce",
   "metadata": {
    "id": "JaQ5oUkRnr7N"
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../dataset/us-patent-phrase-to-phrase-matching/'\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38b47b-7a99-4942-b910-c0e7cac5cebf",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8398a15a-644b-4e83-8d46-543e37ed59d8",
   "metadata": {
    "id": "T-mYdoUjnr7R"
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"USPPPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384aa55c-0f30-4569-9300-3c2a3430727b",
   "metadata": {},
   "source": [
    "## Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00781e09-61ab-49ea-a16e-212fb8683047",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeDb5W-Wnr7U",
    "outputId": "9efeb745-33a8-42c9-a578-3de670bc7cb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2b97f-909f-455b-b81d-b90a366b2676",
   "metadata": {},
   "source": [
    "## Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8e52ee-27e5-4260-90a1-684468fa2599",
   "metadata": {
    "id": "PWOAoAcCnr7W"
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f8751-79fe-492a-a1c8-eb35867f3909",
   "metadata": {
    "id": "IeqIEukjnr7W"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca74519-9ccd-48be-ba61-67466fceea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "      <th>score_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]abatement of pollution[SEP]HUMAN...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]act of abating[SEP]HUMAN NECESSI...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]active catalyst[SEP]HUMAN NECESS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]eliminating process[SEP]HUMAN NE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]forest region[SEP]HUMAN NECESSIT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id     anchor                  target context  \\\n",
       "0           0  37d61fd2272659b1  abatement  abatement of pollution     A47   \n",
       "1           1  7b9652b17b68b7a4  abatement          act of abating     A47   \n",
       "2           2  36d72442aefd8232  abatement         active catalyst     A47   \n",
       "3           3  5296b0c19e1ce60e  abatement     eliminating process     A47   \n",
       "4           4  54c1e3b9184cb5b6  abatement           forest region     A47   \n",
       "\n",
       "   score                                       context_text  \\\n",
       "0   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "1   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "2   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "3   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "4   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "\n",
       "                                                text  score_map  \n",
       "0  abatement[SEP]abatement of pollution[SEP]HUMAN...          2  \n",
       "1  abatement[SEP]act of abating[SEP]HUMAN NECESSI...          3  \n",
       "2  abatement[SEP]active catalyst[SEP]HUMAN NECESS...          1  \n",
       "3  abatement[SEP]eliminating process[SEP]HUMAN NE...          2  \n",
       "4  abatement[SEP]forest region[SEP]HUMAN NECESSIT...          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpc_texts = torch.load('cpc_texts.pth')\n",
    "dataframe = pd.read_csv(\"dataframe.csv\")\n",
    "display(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17be84-7e8e-4cc8-99a5-5b5835d9984c",
   "metadata": {},
   "source": [
    "## Debug Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8036555-0930-4a1e-a4e6-fb088d201ced",
   "metadata": {
    "id": "6zeWhBTmvC2N"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    dataframe = dataframe.iloc[:CFG.debug_samples,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc687c3-b2d5-4ad1-b872-bc08c0edea4e",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f8730b-3f39-46bb-ad8d-28521a3aa08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "      <th>score_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>983</td>\n",
       "      <td>d336ee90bdc74b1c</td>\n",
       "      <td>air flow line</td>\n",
       "      <td>fluid flow line</td>\n",
       "      <td>B63</td>\n",
       "      <td>0.75</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. SHIPS OR ...</td>\n",
       "      <td>air flow line[SEP]fluid flow line[SEP]PERFORMI...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>700</td>\n",
       "      <td>e5a6dccf738babe3</td>\n",
       "      <td>adjacent laterally</td>\n",
       "      <td>adjacent to mall</td>\n",
       "      <td>A41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HUMAN NECESSITIES. WEARING APPAREL</td>\n",
       "      <td>adjacent laterally[SEP]adjacent to mall[SEP]HU...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>463</td>\n",
       "      <td>f030009ac7858e1b</td>\n",
       "      <td>acrylate groups</td>\n",
       "      <td>interpolymer invention</td>\n",
       "      <td>D21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>TEXTILES; PAPER. PAPER-MAKING; PRODUCTION OF C...</td>\n",
       "      <td>acrylate groups[SEP]interpolymer invention[SEP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>926</td>\n",
       "      <td>0136064bfb779543</td>\n",
       "      <td>agitate means</td>\n",
       "      <td>muscle shivering</td>\n",
       "      <td>B01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. PHYSICAL ...</td>\n",
       "      <td>agitate means[SEP]muscle shivering[SEP]PERFORM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>e865c688d8198872</td>\n",
       "      <td>accept information</td>\n",
       "      <td>information data</td>\n",
       "      <td>A45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. HAND OR TRAVELLING ARTICLES</td>\n",
       "      <td>accept information[SEP]information data[SEP]HU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                id              anchor                  target  \\\n",
       "983         983  d336ee90bdc74b1c       air flow line         fluid flow line   \n",
       "700         700  e5a6dccf738babe3  adjacent laterally        adjacent to mall   \n",
       "463         463  f030009ac7858e1b     acrylate groups  interpolymer invention   \n",
       "926         926  0136064bfb779543       agitate means        muscle shivering   \n",
       "235         235  e865c688d8198872  accept information        information data   \n",
       "\n",
       "    context  score                                       context_text  \\\n",
       "983     B63   0.75  PERFORMING OPERATIONS; TRANSPORTING. SHIPS OR ...   \n",
       "700     A41   0.00                 HUMAN NECESSITIES. WEARING APPAREL   \n",
       "463     D21   0.25  TEXTILES; PAPER. PAPER-MAKING; PRODUCTION OF C...   \n",
       "926     B01   0.00  PERFORMING OPERATIONS; TRANSPORTING. PHYSICAL ...   \n",
       "235     A45   0.25     HUMAN NECESSITIES. HAND OR TRAVELLING ARTICLES   \n",
       "\n",
       "                                                  text  score_map  \n",
       "983  air flow line[SEP]fluid flow line[SEP]PERFORMI...          3  \n",
       "700  adjacent laterally[SEP]adjacent to mall[SEP]HU...          0  \n",
       "463  acrylate groups[SEP]interpolymer invention[SEP...          1  \n",
       "926  agitate means[SEP]muscle shivering[SEP]PERFORM...          0  \n",
       "235  accept information[SEP]information data[SEP]HU...          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "      <th>score_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>719</td>\n",
       "      <td>9658a68dedd1b4cc</td>\n",
       "      <td>adjacent laterally</td>\n",
       "      <td>radius</td>\n",
       "      <td>A41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. WEARING APPAREL</td>\n",
       "      <td>adjacent laterally[SEP]radius[SEP]HUMAN NECESS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>256</td>\n",
       "      <td>d2088cdd8be8761b</td>\n",
       "      <td>achieve authentication</td>\n",
       "      <td>biometric</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "      <td>achieve authentication[SEP]biometric[SEP]ELECT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>548</td>\n",
       "      <td>8ed41bd0deb21205</td>\n",
       "      <td>activating position</td>\n",
       "      <td>active material</td>\n",
       "      <td>G06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PHYSICS. COMPUTING; CALCULATING; COUNTING</td>\n",
       "      <td>activating position[SEP]active material[SEP]PH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>473</td>\n",
       "      <td>07f1cfe84cd4ebdc</td>\n",
       "      <td>acrylate groups</td>\n",
       "      <td>nitro group</td>\n",
       "      <td>D21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>TEXTILES; PAPER. PAPER-MAKING; PRODUCTION OF C...</td>\n",
       "      <td>acrylate groups[SEP]nitro group[SEP]TEXTILES; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>308</td>\n",
       "      <td>f798712a28d6660b</td>\n",
       "      <td>acid absorption</td>\n",
       "      <td>rosmarinic acid</td>\n",
       "      <td>B01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. PHYSICAL ...</td>\n",
       "      <td>acid absorption[SEP]rosmarinic acid[SEP]PERFOR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                id                  anchor           target  \\\n",
       "719         719  9658a68dedd1b4cc      adjacent laterally           radius   \n",
       "256         256  d2088cdd8be8761b  achieve authentication        biometric   \n",
       "548         548  8ed41bd0deb21205     activating position  active material   \n",
       "473         473  07f1cfe84cd4ebdc         acrylate groups      nitro group   \n",
       "308         308  f798712a28d6660b         acid absorption  rosmarinic acid   \n",
       "\n",
       "    context  score                                       context_text  \\\n",
       "719     A41   0.25                 HUMAN NECESSITIES. WEARING APPAREL   \n",
       "256     H04   0.25      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE   \n",
       "548     G06   0.00          PHYSICS. COMPUTING; CALCULATING; COUNTING   \n",
       "473     D21   0.00  TEXTILES; PAPER. PAPER-MAKING; PRODUCTION OF C...   \n",
       "308     B01   0.25  PERFORMING OPERATIONS; TRANSPORTING. PHYSICAL ...   \n",
       "\n",
       "                                                  text  score_map  \n",
       "719  adjacent laterally[SEP]radius[SEP]HUMAN NECESS...          1  \n",
       "256  achieve authentication[SEP]biometric[SEP]ELECT...          1  \n",
       "548  activating position[SEP]active material[SEP]PH...          0  \n",
       "473  acrylate groups[SEP]nitro group[SEP]TEXTILES; ...          0  \n",
       "308  acid absorption[SEP]rosmarinic acid[SEP]PERFOR...          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dataframe, test_size = 0.1, random_state = CFG.seed, stratify = dataframe.score_map)\n",
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb4ea4-8f8b-4860-bccf-238f968b3c3e",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c5624-94b1-4170-b3ea-5656ca92c3f4",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95fee091-3c88-4e80-b6ba-16dc97f07200",
   "metadata": {
    "id": "Vs9TXF3pufI0"
   },
   "outputs": [],
   "source": [
    "def set_tokenizer(config_dict):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config_dict['model'])\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "    config_dict['tokenizer'] = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74b765-ebb0-4d17-8c23-ad9d7bfaa153",
   "metadata": {
    "id": "I8CsgvQFnr7e"
   },
   "source": [
    "## Maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4147285a-c73b-4329-8eeb-d862bf599d29",
   "metadata": {
    "id": "Hxle-CPRnr7e"
   },
   "outputs": [],
   "source": [
    "def set_max_len(config_dict, cpc_texts=cpc_texts, train_df=dataframe):\n",
    "    tokenizer = config_dict['tokenizer']\n",
    "    lengths_dict = {}\n",
    "\n",
    "    lengths = []\n",
    "    tk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    lengths_dict['context_text'] = lengths\n",
    "\n",
    "    for text_col in ['anchor', 'target']:\n",
    "        lengths = []\n",
    "        tk0 = tqdm(train_df[text_col].fillna(\"\").values, total=len(train_df))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            lengths.append(length)\n",
    "        lengths_dict[text_col] = lengths\n",
    "\n",
    "    config_dict['max_len'] = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n",
    "                    + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cef85f-21a3-4a36-8982-fdf33b97e362",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66398e59-885f-49ef-8f6b-79b5518c7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(config_dict, text):\n",
    "    tokenizer = config_dict['tokenizer']\n",
    "    inputs = tokenizer(text,\n",
    "                       add_special_tokens = True,\n",
    "                       max_length = config_dict['max_len'],\n",
    "                       padding = \"max_length\",\n",
    "                       return_offsets_mapping = False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5087433-6899-4564-9194-537a0164aefd",
   "metadata": {
    "id": "AEyPtCQvnr7f"
   },
   "outputs": [],
   "source": [
    "class USPPM_dataset(Dataset):\n",
    "    def __init__(self, config_dict, train_df, train=True):\n",
    "        self.config_dict = config_dict\n",
    "        self.texts = train_df['text'].values\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.labels = train_df['score'].values\n",
    "            self.score_map = train_df['score_map'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.config_dict, self.texts[item])\n",
    "        if self.train:\n",
    "            labels = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "            return dict(\n",
    "                  inputs = inputs,\n",
    "                  labels = labels\n",
    "            )\n",
    "        else:\n",
    "            return dict(\n",
    "                  inputs = inputs\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f2468-de35-4bd7-87e8-8b83a3794ab1",
   "metadata": {},
   "source": [
    "# K-Fold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18fb2d-a3f7-4cfb-a3dc-ad613c26c2fb",
   "metadata": {
    "id": "Q0HgRiNlnr7g"
   },
   "source": [
    "## KFold DataModule definition                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a3a977-1f80-4039-abfc-eb24ebd4e9a2",
   "metadata": {
    "id": "MVmHarn0nr7h"
   },
   "outputs": [],
   "source": [
    "class BaseKFoldDataModule(LightningDataModule, ABC):\n",
    "    @abstractmethod\n",
    "    def setup_folds(self, num_folds: int) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_fold_index(self, fold_index: int) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d242c6a-01f3-4388-a38b-47f85fcd2240",
   "metadata": {
    "id": "4wjghxElnr7i"
   },
   "source": [
    "## KFoldDataModule implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f986de2-1661-4654-8db0-348e7d8174c1",
   "metadata": {
    "id": "BUeE7TJUnr7i"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class USPPPM_kf_datamodule(BaseKFoldDataModule):\n",
    "    def __init__(self, config_dict, dataframe = dataframe):\n",
    "        \n",
    "        self.config_dict = config_dict\n",
    "        self.prepare_data_per_node = False\n",
    "        self._log_hyperparams = False\n",
    "        \n",
    "        train_dataset: Optional[Dataset] = None\n",
    "        test_dataset: Optional[Dataset] = None\n",
    "        train_fold: Optional[Dataset] = None\n",
    "        val_fold: Optional[Dataset] = None\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "            \n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        train_df, test_df = train_test_split(self.dataframe, test_size = 0.1, random_state = CFG.seed, stratify = self.dataframe.score_map)\n",
    "        self.train_dataset = USPPM_dataset(self.config_dict, train_df)\n",
    "        self.test_dataset = USPPM_dataset(self.config_dict, test_df)\n",
    "\n",
    "    def setup_folds(self, num_folds: int) -> None:\n",
    "        self.num_folds = num_folds\n",
    "        Fold = StratifiedKFold(n_splits=self.num_folds, shuffle=True)\n",
    "        self.splits = [split for split in Fold.split(self.train_dataset, self.train_dataset.score_map)]\n",
    "\n",
    "    def setup_fold_index(self, fold_index: int) -> None:\n",
    "        train_indices, val_indices = self.splits[fold_index]\n",
    "        self.train_fold = Subset(self.train_dataset, train_indices)\n",
    "        self.val_fold = Subset(self.train_dataset, val_indices)\n",
    "        print(\"TRAIN FOLD\", fold_index + 1, len(self.train_fold))\n",
    "        print(\"VALID FOLD\", fold_index + 1, len(self.val_fold))\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_fold, num_workers = self.config_dict['num_workers'], batch_size = self.config_dict['batch_size'])\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_fold, num_workers = self.config_dict['num_workers'], batch_size = self.config_dict['batch_size'])\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, num_workers = self.config_dict['num_workers'], batch_size = self.config_dict['batch_size'])\n",
    "    \n",
    "    def __post_init__(cls):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275e1d1-51fd-4d78-a9f1-4b8dde805ab0",
   "metadata": {
    "id": "1RLF9XVinr7j"
   },
   "source": [
    "## Ensemble Model for kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aebb0fff-b740-4997-9ebd-1624d561d9af",
   "metadata": {
    "id": "dwvwouh3nr7j"
   },
   "outputs": [],
   "source": [
    "class EnsembleVotingModel(LightningModule):\n",
    "    def __init__(self, model_cls: Type[LightningModule], checkpoint_paths: List[str]):\n",
    "        super().__init__()\n",
    "        # Create `num_folds` models with their associated fold weights\n",
    "        self.models = torch.nn.ModuleList([model_cls.load_from_checkpoint(p) for p in checkpoint_paths])\n",
    "        self.last_acc = Accuracy()\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        # Compute the averaged predictions over the `num_folds` models.\n",
    "        logits = torch.stack([m(batch[0]) for m in self.models])\n",
    "\n",
    "        avg_logits = logits.mean(0)\n",
    "        acc = self.last_acc(avg_logits, batch[1])\n",
    "\n",
    "        accs = torch.stack([self.last_acc(logit, batch[1]) for logit in logits])\n",
    "        avg_acc = accs.mean(0)\n",
    "        self.log('voting acc', acc)\n",
    "        print('accs print', accs)\n",
    "        print('avg_acc print', avg_acc)\n",
    "        self.log('avg_acc', avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea969f-f0e2-4771-8460-f5df58d3aa7e",
   "metadata": {
    "id": "E2ch26C4nr7k"
   },
   "source": [
    "## KFoldLoop implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdcaf77c-dc40-4149-9a17-6b297691c26c",
   "metadata": {
    "id": "ozwbPnfenr7m"
   },
   "outputs": [],
   "source": [
    "class KFoldLoop(Loop):\n",
    "    def __init__(self, num_folds: int, export_path: str) -> None:\n",
    "        super().__init__()\n",
    "        self.num_folds = num_folds\n",
    "        self.current_fold: int = 0\n",
    "        self.export_path = export_path\n",
    "\n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.current_fold >= self.num_folds\n",
    "\n",
    "    def connect(self, fit_loop: FitLoop) -> None:\n",
    "        self.fit_loop = fit_loop\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Nothing to reset in this loop.\"\"\"\n",
    "\n",
    "    def on_run_start(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Used to call `setup_folds` from the `BaseKFoldDataModule` instance and store the original weights of the model.\"\"\"\n",
    "        assert isinstance(self.trainer.datamodule, BaseKFoldDataModule)\n",
    "        self.trainer.datamodule.setup_folds(self.num_folds)\n",
    "        self.lightning_module_state_dict = deepcopy(self.trainer.lightning_module.state_dict())\n",
    "\n",
    "    def on_advance_start(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Used to call `setup_fold_index` from the `BaseKFoldDataModule` instance.\"\"\"\n",
    "        print(f\"STARTING FOLD {self.current_fold+1}\")\n",
    "        assert isinstance(self.trainer.datamodule, BaseKFoldDataModule)\n",
    "        self.trainer.datamodule.setup_fold_index(self.current_fold)\n",
    "\n",
    "    def advance(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Used to the run a fitting and testing on the current hold.\"\"\"\n",
    "        self._reset_fitting()  # requires to reset the tracking stage.\n",
    "        self.fit_loop.run()\n",
    "\n",
    "        self._reset_testing()  # requires to reset the tracking stage.\n",
    "        self.trainer.test_loop.run()\n",
    "        print('TEST for FOLD', self.current_fold+1)\n",
    "        \n",
    "        self.current_fold += 1  # increment fold tracking number.\n",
    "\n",
    "    def on_advance_end(self) -> None:\n",
    "        \"\"\"Used to save the weights of the current fold and reset the LightningModule and its optimizers.\"\"\"\n",
    "        self.trainer.save_checkpoint(os.path.join(self.export_path, f\"model.{self.current_fold}.pt\"))\n",
    "        # restore the original weights + optimizers and schedulers.\n",
    "        self.trainer.lightning_module.load_state_dict(self.lightning_module_state_dict)\n",
    "        self.trainer.strategy.setup_optimizers(self.trainer)\n",
    "        self.replace(fit_loop=FitLoop)\n",
    "\n",
    "    def on_run_end(self) -> None:\n",
    "        \"\"\"Used to compute the performance of the ensemble model on the test set.\"\"\"\n",
    "        checkpoint_paths = [os.path.join(self.export_path, f\"model.{f_idx + 1}.pt\") for f_idx in range(self.num_folds)]\n",
    "        voting_model = EnsembleVotingModel(type(self.trainer.lightning_module), checkpoint_paths)\n",
    "        voting_model.trainer = self.trainer\n",
    "\n",
    "        # This requires to connect the new model and move it the right device.\n",
    "        self.trainer.strategy.connect(voting_model)\n",
    "        self.trainer.strategy.model_to_device()\n",
    "        self.trainer.test_loop.run()\n",
    "\n",
    "    def on_save_checkpoint(self) -> Dict[str, int]:\n",
    "        return {\"current_fold\": self.current_fold}\n",
    "\n",
    "    def on_load_checkpoint(self, state_dict: Dict) -> None:\n",
    "        self.current_fold = state_dict[\"current_fold\"]\n",
    "\n",
    "    def _reset_fitting(self) -> None:\n",
    "        self.trainer.reset_train_dataloader()\n",
    "        self.trainer.reset_val_dataloader()\n",
    "        self.trainer.state.fn = TrainerFn.FITTING\n",
    "        self.trainer.training = True\n",
    "\n",
    "    def _reset_testing(self) -> None:\n",
    "        self.trainer.reset_test_dataloader()\n",
    "        self.trainer.state.fn = TrainerFn.TESTING\n",
    "        self.trainer.testing = True\n",
    "\n",
    "    def __getattr__(self, key) -> Any:\n",
    "        # requires to be overridden as attributes of the wrapped loop are being accessed.\n",
    "        if key not in self.__dict__:\n",
    "            return getattr(self.fit_loop, key)\n",
    "        return self.__dict__[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b6702-c290-42e7-8055-33f48a348dff",
   "metadata": {
    "id": "GYPJGJ-Fnr7n"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8ff48d-c3de-4e84-8df6-b064ed250e6f",
   "metadata": {
    "id": "GY0QVy62nr7o"
   },
   "outputs": [],
   "source": [
    "class USPPPM_model(pl.LightningModule):\n",
    "    def __init__(self, config_dict=config_dict, config_path=None, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(config_dict['model'], output_hidden_states = True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        self.config_dict = config_dict\n",
    "        self.n_warmup_steps = config_dict['warmup_steps']\n",
    "        self.n_training_steps = config_dict['training_steps']\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_dict['model'], config = self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "            \n",
    "        self.fc_dropout = nn.Dropout(config_dict['fc_dropout'])\n",
    "        self.fc = nn.Linear(self.config.hidden_size, config_dict['target_size'])\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.batch_labels = []\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(inputs, labels.unsqueeze(1))\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        # session.report({\"train_loss\": loss})  # Send the score to Tune.\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(inputs, labels.unsqueeze(1))\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        # session.report({\"val_loss\": loss})  # Send the score to Tune.\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(inputs, labels.unsqueeze(1))\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        # session.report({\"test_loss\": loss})  # Send the score to Tune.\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_epoch_end(self, batch_results):\n",
    "        outputs, labels, losses = [], [], []\n",
    "        for batch in batch_results:\n",
    "            outputs.append(batch['predictions'])\n",
    "            labels.append(batch['labels'])\n",
    "            losses.append(batch['loss'])\n",
    "\n",
    "        labels = torch.cat(labels).cpu().numpy()\n",
    "        predictions = np.concatenate(torch.cat(outputs).sigmoid().to('cpu').numpy())\n",
    "        score = get_score(labels, predictions)\n",
    "        self.log(\"val_score\", score, prog_bar=True, logger=True)\n",
    "        # tune.report({\"val_score\": score})  # Send the score to Tune.\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config_dict['encoder_lr'])\n",
    "        # optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "          optimizer=optimizer,\n",
    "          lr_scheduler=dict(\n",
    "            scheduler=scheduler,\n",
    "            interval='step'\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563fc01-1170-43cf-ac1b-f577730cf1a6",
   "metadata": {
    "id": "fK6gpNx1nr7q"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b10ba-6205-419f-844f-0fe5b9dc07d0",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f7ea00-b260-41fd-a636-159e38b66fa8",
   "metadata": {
    "id": "fAEhe31znr7q"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best_checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "metrics = {\"val_score\": \"val_score\", \"train_loss\" : \"train_loss\", \"val_loss\" : \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "143d9932-bb80-433c-b534-9464d9729443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config_dict):  # Pass a \"config\" dictionary into your trainable.\n",
    "\n",
    "    steps_per_epoch = len(train_df) // config_dict['batch_size']\n",
    "    config_dict['training_steps'] = steps_per_epoch * config_dict['epochs']\n",
    "    \n",
    "    set_tokenizer(config_dict)\n",
    "    set_max_len(config_dict)\n",
    "    # train_dataset = USPPM_dataset(config_dict)\n",
    "    datamodule = USPPPM_kf_datamodule(config_dict, dataframe)\n",
    "    \n",
    "    model = USPPPM_model(config_dict)\n",
    "    \n",
    "    callbacks = [TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "    trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            check_val_every_n_epoch=1,\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=config_dict['epochs'],\n",
    "            devices=[1],\n",
    "            accelerator=\"gpu\",\n",
    "            )\n",
    "    \n",
    "    internal_fit_loop = trainer.fit_loop\n",
    "    trainer.fit_loop = KFoldLoop(config_dict['n_fold'], export_path=\"./\")\n",
    "    trainer.fit_loop.connect(internal_fit_loop)\n",
    "    \n",
    "    trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ad5bea5-2224-4ef3-b06c-21db07e5a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(tune.with_resources(trainable, \n",
    "                                       {\"gpu\": 4}), \n",
    "                                       param_space = config_dict,\n",
    "                                       tune_config = tune.TuneConfig(metric=\"val_score\", mode=\"max\"),\n",
    "                                       # tune_config = tune.TuneConfig(metric=\"val_score\", mode=\"max\"),\n",
    "                                       run_config = air.RunConfig(name=\"tune_uspppm\", verbose=3)\n",
    "                                      )\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea534b6-3f59-4342-8e6a-738bb2f14ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:03:47,053\tWARNING function_trainable.py:619 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-06 20:05:25 (running for 00:01:38.29)<br>Memory usage on this node: 88.8/503.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/80 CPUs, 0/4 GPUs, 0.0/288.26 GiB heap, 0.0/127.53 GiB objects (0.0/1.0 accelerator_type:P100)<br>Current best trial: bdd23_00000 with val_score=0.08050053239597282 and parameters={'target_size': 1, 'num_workers': 8, 'batch_size': 64, 'epochs': 2, 'n_fold': 2, 'warmup_steps': 0, 'min_lr': 1e-06, 'encoder_lr': 2e-05, 'decoder_lr': 2e-05, 'eps': 1e-06, 'betas': (0.9, 0.999), 'weight_decay': 0.01, 'fc_dropout': 0.2, 'model': 'distilbert-base-uncased', 'training_steps': 28, 'tokenizer': PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), 'max_len': 89}<br>Result logdir: /storagenfs/m.petix/ray_results/tune_uspppm<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_score</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trainable_bdd23_00000</td><td>ERROR   </td><td>131.114.50.210:362371</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         65.7888</td><td style=\"text-align: right;\">  0.0805005</td><td style=\"text-align: right;\">    0.686017</td><td style=\"text-align: right;\">  0.669862</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                       </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trainable_bdd23_00000</td><td style=\"text-align: right;\">           1</td><td>/storagenfs/m.petix/ray_results/tune_uspppm/trainable_bdd23_00000_0_2022-11-06_20-03-47/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:00<00:00, 4578.24it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14155.93it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 12903.33it/s]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m Missing logger folder: lightning_logs/USPPPM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m STARTING FOLD 1\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TRAIN FOLD 1 450\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m VALID FOLD 1 450\n",
      "Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   | Name       | Type              | Params\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m -------------------------------------------------\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 0 | criterion  | BCEWithLogitsLoss | 0     \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 1 | model      | DistilBertModel   | 66.4 M\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 2 | fc_dropout | Dropout           | 0     \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 3 | fc         | Linear            | 769   \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 4 | attention  | Sequential        | 394 K \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m -------------------------------------------------\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 66.8 M    Trainable params\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 66.8 M    Total params\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 267.032   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m /storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▋         | 1/16 [00:01<00:15,  1.04s/it, loss=0.691, v_num=0, train_loss=0.691]\n",
      "Epoch 0:  12%|█▎        | 2/16 [00:01<00:09,  1.53it/s, loss=0.681, v_num=0, train_loss=0.671]\n",
      "Epoch 0:  19%|█▉        | 3/16 [00:01<00:06,  1.92it/s, loss=0.671, v_num=0, train_loss=0.650]\n",
      "Epoch 0:  25%|██▌       | 4/16 [00:01<00:05,  2.19it/s, loss=0.675, v_num=0, train_loss=0.688]\n",
      "Epoch 0:  31%|███▏      | 5/16 [00:02<00:04,  2.40it/s, loss=0.668, v_num=0, train_loss=0.639]\n",
      "Epoch 0:  38%|███▊      | 6/16 [00:02<00:03,  2.56it/s, loss=0.661, v_num=0, train_loss=0.626]\n",
      "Epoch 0:  50%|█████     | 8/16 [00:02<00:02,  3.02it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  56%|█████▋    | 9/16 [00:03<00:02,  2.84it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  62%|██████▎   | 10/16 [00:03<00:01,  3.08it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "Epoch 0:  69%|██████▉   | 11/16 [00:03<00:01,  3.31it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  88%|████████▊ | 14/16 [00:03<00:00,  3.92it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "Epoch 0:  94%|█████████▍| 15/16 [00:03<00:00,  4.10it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0: 100%|██████████| 16/16 [00:03<00:00,  4.28it/s, loss=0.677, v_num=0, train_loss=0.755]\n",
      "Result for trainable_bdd23_00000:\n",
      "  date: 2022-11-06_20-04-06\n",
      "  done: false\n",
      "  experiment_id: 22f475b356ee4d0f94b7d54f8e5c17e3\n",
      "  hostname: c4130-p100\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 131.114.50.210\n",
      "  pid: 362371\n",
      "  time_since_restore: 10.971570014953613\n",
      "  time_this_iter_s: 10.971570014953613\n",
      "  time_total_s: 10.971570014953613\n",
      "  timestamp: 1667761446\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 0.7552312612533569\n",
      "  training_iteration: 1\n",
      "  trial_id: bdd23_00000\n",
      "  val_loss: 0.646181583404541\n",
      "  val_score: 0.1075670076804115\n",
      "  warmup_time: 0.0051670074462890625\n",
      "  \n",
      "Epoch 0: 100%|██████████| 16/16 [00:03<00:00,  4.17it/s, loss=0.677, v_num=0, train_loss=0.755, val_loss=0.646, val_score=0.108]\n",
      "Epoch 0: 100%|██████████| 16/16 [00:03<00:00,  4.16it/s, loss=0.677, v_num=0, train_loss=0.755, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s, loss=0.677, v_num=0, train_loss=0.755, val_loss=0.646, val_score=0.108]         \n",
      "Epoch 1:   6%|▋         | 1/16 [00:00<00:13,  1.13it/s, loss=0.672, v_num=0, train_loss=0.627, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  12%|█▎        | 2/16 [00:01<00:08,  1.74it/s, loss=0.67, v_num=0, train_loss=0.655, val_loss=0.646, val_score=0.108] \n",
      "Epoch 1:  19%|█▉        | 3/16 [00:01<00:06,  2.13it/s, loss=0.666, v_num=0, train_loss=0.626, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  25%|██▌       | 4/16 [00:01<00:05,  2.38it/s, loss=0.668, v_num=0, train_loss=0.684, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  31%|███▏      | 5/16 [00:01<00:04,  2.58it/s, loss=0.664, v_num=0, train_loss=0.626, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  38%|███▊      | 6/16 [00:02<00:03,  2.73it/s, loss=0.662, v_num=0, train_loss=0.624, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  44%|████▍     | 7/16 [00:02<00:03,  2.84it/s, loss=0.663, v_num=0, train_loss=0.681, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  50%|█████     | 8/16 [00:02<00:02,  3.21it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 9/16 [00:03<00:02,  2.89it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  62%|██████▎   | 10/16 [00:03<00:01,  3.13it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  69%|██████▉   | 11/16 [00:03<00:01,  3.36it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  75%|███████▌  | 12/16 [00:03<00:01,  3.57it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  81%|████████▏ | 13/16 [00:03<00:00,  3.78it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "Epoch 1:  88%|████████▊ | 14/16 [00:03<00:00,  3.97it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  94%|█████████▍| 15/16 [00:03<00:00,  4.16it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.108]\n",
      "Result for trainable_bdd23_00000:\n",
      "  date: 2022-11-06_20-04-20\n",
      "  done: false\n",
      "  experiment_id: 22f475b356ee4d0f94b7d54f8e5c17e3\n",
      "  hostname: c4130-p100\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 131.114.50.210\n",
      "  pid: 362371\n",
      "  time_since_restore: 25.162285566329956\n",
      "  time_this_iter_s: 14.190715551376343\n",
      "  time_total_s: 25.162285566329956\n",
      "  timestamp: 1667761460\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 0.7368876934051514\n",
      "  training_iteration: 2\n",
      "  trial_id: bdd23_00000\n",
      "  val_loss: 0.645910918712616\n",
      "  val_score: 0.11003900642702776\n",
      "  warmup_time: 0.0051670074462890625\n",
      "  \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  4.25it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.110]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  4.25it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.110]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:14<00:00,  1.08it/s, loss=0.668, v_num=0, train_loss=0.737, val_loss=0.646, val_score=0.110]\n",
      "Testing: 0it [00:00, ?it/s]371)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 57.41it/s]\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 14.75it/s]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m │         test_loss         │    0.6477512121200562     │\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m └───────────────────────────┴───────────────────────────┘\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TEST for FOLD 1\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m STARTING FOLD 2\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TRAIN FOLD 2 450\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m VALID FOLD 2 450\n",
      "Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▋         | 1/16 [00:00<00:11,  1.28it/s, loss=0.669, v_num=0, train_loss=0.694, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  12%|█▎        | 2/16 [00:01<00:07,  1.92it/s, loss=0.67, v_num=0, train_loss=0.687, val_loss=0.646, val_score=0.110, test_loss=0.648] \n",
      "Epoch 0:  19%|█▉        | 3/16 [00:01<00:05,  2.30it/s, loss=0.671, v_num=0, train_loss=0.686, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  25%|██▌       | 4/16 [00:01<00:04,  2.55it/s, loss=0.672, v_num=0, train_loss=0.687, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  31%|███▏      | 5/16 [00:01<00:04,  2.73it/s, loss=0.671, v_num=0, train_loss=0.684, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  38%|███▊      | 6/16 [00:02<00:03,  2.86it/s, loss=0.672, v_num=0, train_loss=0.677, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  50%|█████     | 8/16 [00:02<00:02,  3.35it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  56%|█████▋    | 9/16 [00:03<00:02,  2.97it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  62%|██████▎   | 10/16 [00:03<00:01,  3.22it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  69%|██████▉   | 11/16 [00:03<00:01,  3.45it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  81%|████████▏ | 13/16 [00:03<00:00,  3.88it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Epoch 0:  88%|████████▊ | 14/16 [00:03<00:00,  4.07it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 15/16 [00:03<00:00,  4.26it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.646, val_score=0.110, test_loss=0.648]\n",
      "Result for trainable_bdd23_00000:\n",
      "  date: 2022-11-06_20-04-46\n",
      "  done: false\n",
      "  experiment_id: 22f475b356ee4d0f94b7d54f8e5c17e3\n",
      "  hostname: c4130-p100\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 131.114.50.210\n",
      "  pid: 362371\n",
      "  time_since_restore: 51.06911325454712\n",
      "  time_this_iter_s: 25.906827688217163\n",
      "  time_total_s: 51.06911325454712\n",
      "  timestamp: 1667761486\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 0.7174028158187866\n",
      "  training_iteration: 3\n",
      "  trial_id: bdd23_00000\n",
      "  val_loss: 0.6734946966171265\n",
      "  val_score: 0.07464388697223191\n",
      "  warmup_time: 0.0051670074462890625\n",
      "  \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 0: 100%|██████████| 16/16 [00:03<00:00,  4.36it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 0: 100%|██████████| 16/16 [00:03<00:00,  4.36it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s, loss=0.674, v_num=0, train_loss=0.717, val_loss=0.673, val_score=0.0746, test_loss=0.648]         \n",
      "Epoch 1:   6%|▋         | 1/16 [00:00<00:13,  1.11it/s, loss=0.676, v_num=0, train_loss=0.677, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  12%|█▎        | 2/16 [00:01<00:08,  1.72it/s, loss=0.678, v_num=0, train_loss=0.671, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  19%|█▉        | 3/16 [00:01<00:06,  2.09it/s, loss=0.676, v_num=0, train_loss=0.660, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  25%|██▌       | 4/16 [00:01<00:05,  2.36it/s, loss=0.672, v_num=0, train_loss=0.672, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  31%|███▏      | 5/16 [00:01<00:04,  2.56it/s, loss=0.675, v_num=0, train_loss=0.675, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  38%|███▊      | 6/16 [00:02<00:03,  2.70it/s, loss=0.675, v_num=0, train_loss=0.667, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  44%|████▍     | 7/16 [00:02<00:03,  2.83it/s, loss=0.677, v_num=0, train_loss=0.667, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  50%|█████     | 8/16 [00:02<00:02,  3.19it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 9/16 [00:03<00:02,  2.87it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  62%|██████▎   | 10/16 [00:03<00:01,  3.11it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  69%|██████▉   | 11/16 [00:03<00:01,  3.34it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1:  75%|███████▌  | 12/16 [00:03<00:01,  3.55it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  81%|████████▏ | 13/16 [00:03<00:00,  3.76it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  88%|████████▊ | 14/16 [00:03<00:00,  3.95it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m \n",
      "Epoch 1:  94%|█████████▍| 15/16 [00:03<00:00,  4.14it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  4.31it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.673, val_score=0.0746, test_loss=0.648]\n",
      "Result for trainable_bdd23_00000:\n",
      "  date: 2022-11-06_20-05-00\n",
      "  done: false\n",
      "  experiment_id: 22f475b356ee4d0f94b7d54f8e5c17e3\n",
      "  hostname: c4130-p100\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 131.114.50.210\n",
      "  pid: 362371\n",
      "  time_since_restore: 65.7887601852417\n",
      "  time_this_iter_s: 14.71964693069458\n",
      "  time_total_s: 65.7887601852417\n",
      "  timestamp: 1667761500\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 0.6860167980194092\n",
      "  training_iteration: 4\n",
      "  trial_id: bdd23_00000\n",
      "  val_loss: 0.6698618531227112\n",
      "  val_score: 0.08050053239597282\n",
      "  warmup_time: 0.0051670074462890625\n",
      "  \n",
      "Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  4.24it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.670, val_score=0.0805, test_loss=0.648]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  4.24it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.670, val_score=0.0805, test_loss=0.648]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:15<00:00,  1.05it/s, loss=0.677, v_num=0, train_loss=0.686, val_loss=0.670, val_score=0.0805, test_loss=0.648]\n",
      "Testing: 0it [00:00, ?it/s]371)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 67.54it/s]\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 14.84it/s]\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m │         test_loss         │    0.6688225269317627     │\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m └───────────────────────────┴───────────────────────────┘\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TEST for FOLD 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m 2022-11-06 20:05:25,111\tERROR function_trainable.py:298 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 289, in run\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 362, in entrypoint\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     return self._trainable_func(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 684, in _trainable_func\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"<ipython-input-23-c3c57964d798>\", line 28, in trainable\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     self._call_and_handle_interrupt(\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     results = self._run_stage()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     return self._run_train()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     self.fit_loop.run()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 207, in run\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m     output = self.on_run_end()\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"<ipython-input-20-ab063a5a18c0>\", line 52, in on_run_end\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"<ipython-input-19-e7c85a1176f5>\", line 5, in __init__\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m   File \"<ipython-input-19-e7c85a1176f5>\", line 5, in <listcomp>\n",
      "\u001b[2m\u001b[36m(trainable pid=362371)\u001b[0m TypeError: load_from_checkpoint() missing 1 required positional argument: 'checkpoint_path'\n",
      "2022-11-06 20:05:25,218\tERROR trial_runner.py:987 -- Trial trainable_bdd23_00000: Error processing event.\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=362371, ip=131.114.50.210, repr=trainable)\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 349, in train\n",
      "    result = self.step()\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 417, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 589, in _report_thread_runner_error\n",
      "    raise e\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 289, in run\n",
      "    self._entrypoint()\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 362, in entrypoint\n",
      "    return self._trainable_func(\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py\", line 684, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"<ipython-input-23-c3c57964d798>\", line 28, in trainable\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n",
      "    return self._run_train()\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 207, in run\n",
      "    output = self.on_run_end()\n",
      "  File \"<ipython-input-20-ab063a5a18c0>\", line 52, in on_run_end\n",
      "  File \"<ipython-input-19-e7c85a1176f5>\", line 5, in __init__\n",
      "  File \"<ipython-input-19-e7c85a1176f5>\", line 5, in <listcomp>\n",
      "TypeError: load_from_checkpoint() missing 1 required positional argument: 'checkpoint_path'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for trainable_bdd23_00000:\n",
      "  date: 2022-11-06_20-05-00\n",
      "  done: false\n",
      "  experiment_id: 22f475b356ee4d0f94b7d54f8e5c17e3\n",
      "  experiment_tag: '0'\n",
      "  hostname: c4130-p100\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 131.114.50.210\n",
      "  pid: 362371\n",
      "  time_since_restore: 65.7887601852417\n",
      "  time_this_iter_s: 14.71964693069458\n",
      "  time_total_s: 65.7887601852417\n",
      "  timestamp: 1667761500\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 0.6860167980194092\n",
      "  training_iteration: 4\n",
      "  trial_id: bdd23_00000\n",
      "  val_loss: 0.6698618531227112\n",
      "  val_score: 0.08050053239597282\n",
      "  warmup_time: 0.0051670074462890625\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:05:25,527\tERROR tune.py:754 -- Trials did not complete: [trainable_bdd23_00000]\n",
      "2022-11-06 20:05:25,528\tINFO tune.py:758 -- Total run time: 98.48 seconds (98.17 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(metrics={'val_score': 0.08050053239597282, 'train_loss': 0.6860167980194092, 'val_loss': 0.6698618531227112, 'done': False, 'trial_id': 'bdd23_00000', 'experiment_tag': '0'}, error=RayTaskError(TypeError)(TypeError(\"load_from_checkpoint() missing 1 required positional argument: 'checkpoint_path'\")), log_dir=PosixPath('/storagenfs/m.petix/ray_results/tune_uspppm/trainable_bdd23_00000_0_2022-11-06_20-03-47'))\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result()  # Get best result object\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00d41de4-a568-4693-8519-cd1bb2d02a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
